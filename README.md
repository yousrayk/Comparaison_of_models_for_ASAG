This code investigates the incorporation of Language Models (LLMs) into the realm of Automatic Short Answer Grading (ASAG). We employ pre-trained embeddings from transfer learning models—ELMo, BERT, and GPT-2—to evaluate their efficacy in comparison to existing methodologies. Through the training process on a singular feature, namely cosine similarity, derived from these models, we evaluate RMSE scores and correlation measurements. The results uncover that, notably on the Mohler dataset, ELMo demonstrates superior performance compared to the other three models.
